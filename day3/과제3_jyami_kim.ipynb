{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-nomic in /usr/local/lib/python3.11/site-packages (0.1.2)\n",
      "Requirement already satisfied: langchain_community in /usr/local/lib/python3.11/site-packages (0.2.5)\n",
      "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/site-packages (0.7.0)\n",
      "Requirement already satisfied: langchainhub in /usr/local/lib/python3.11/site-packages (0.1.20)\n",
      "Requirement already satisfied: chromadb in /usr/local/lib/python3.11/site-packages (0.5.2)\n",
      "Requirement already satisfied: langchain in /usr/local/lib/python3.11/site-packages (0.2.5)\n",
      "Requirement already satisfied: langgraph in /usr/local/lib/python3.11/site-packages (0.0.69)\n",
      "Requirement already satisfied: tavily-python in /usr/local/lib/python3.11/site-packages (0.3.3)\n",
      "Requirement already satisfied: langchain-text-splitters in /usr/local/lib/python3.11/site-packages (0.2.1)\n",
      "Requirement already satisfied: gpt4all in /usr/local/lib/python3.11/site-packages (2.7.0)\n",
      "Requirement already satisfied: arxiv in /usr/local/lib/python3.11/site-packages (2.1.0)\n",
      "Requirement already satisfied: langchain-core<0.3,>=0.1.46 in /usr/local/lib/python3.11/site-packages (from langchain-nomic) (0.2.8)\n",
      "Requirement already satisfied: nomic<4.0.0,>=3.0.29 in /usr/local/lib/python3.11/site-packages (from langchain-nomic) (3.0.33)\n",
      "Requirement already satisfied: pillow<11.0.0,>=10.3.0 in /usr/local/lib/python3.11/site-packages (from langchain-nomic) (10.3.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/site-packages (from langchain_community) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/site-packages (from langchain_community) (2.0.30)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/site-packages (from langchain_community) (3.9.5)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/site-packages (from langchain_community) (0.6.7)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.11/site-packages (from langchain_community) (0.1.79)\n",
      "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.11/site-packages (from langchain_community) (1.26.4)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/site-packages (from langchain_community) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.11/site-packages (from langchain_community) (8.4.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/site-packages (from tiktoken) (2024.5.15)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /Users/jyami/Library/Python/3.11/lib/python/site-packages (from langchainhub) (24.1)\n",
      "Requirement already satisfied: types-requests<3.0.0.0,>=2.31.0.2 in /usr/local/lib/python3.11/site-packages (from langchainhub) (2.32.0.20240602)\n",
      "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.11/site-packages (from chromadb) (1.2.1)\n",
      "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.11/site-packages (from chromadb) (2.7.4)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.3 in /usr/local/lib/python3.11/site-packages (from chromadb) (0.7.3)\n",
      "Requirement already satisfied: fastapi>=0.95.2 in /usr/local/lib/python3.11/site-packages (from chromadb) (0.111.0)\n",
      "Requirement already satisfied: uvicorn[standard]>=0.18.3 in /usr/local/lib/python3.11/site-packages (from chromadb) (0.30.1)\n",
      "Requirement already satisfied: posthog>=2.4.0 in /usr/local/lib/python3.11/site-packages (from chromadb) (3.5.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /Users/jyami/Library/Python/3.11/lib/python/site-packages (from chromadb) (4.12.2)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.11/site-packages (from chromadb) (1.18.0)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.11/site-packages (from chromadb) (1.25.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.11/site-packages (from chromadb) (1.25.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /usr/local/lib/python3.11/site-packages (from chromadb) (0.46b0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.11/site-packages (from chromadb) (1.25.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.11/site-packages (from chromadb) (0.19.1)\n",
      "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.11/site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.11/site-packages (from chromadb) (4.66.4)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.11/site-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/site-packages (from chromadb) (6.4.0)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/site-packages (from chromadb) (1.64.1)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.11/site-packages (from chromadb) (4.1.3)\n",
      "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/site-packages (from chromadb) (0.12.3)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.11/site-packages (from chromadb) (30.1.0)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.11/site-packages (from chromadb) (4.1.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/site-packages (from chromadb) (3.10.5)\n",
      "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/site-packages (from chromadb) (0.27.0)\n",
      "Requirement already satisfied: feedparser==6.0.10 in /usr/local/lib/python3.11/site-packages (from arxiv) (6.0.10)\n",
      "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.11/site-packages (from feedparser==6.0.10->arxiv) (1.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/site-packages (from requests<3,>=2->langchain_community) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/site-packages (from requests<3,>=2->langchain_community) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/site-packages (from requests<3,>=2->langchain_community) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/site-packages (from requests<3,>=2->langchain_community) (2024.2.2)\n",
      "Collecting nvidia-cuda-runtime-cu12\n",
      "  Using cached nvidia-cuda-runtime-cu12-0.0.1.dev5.tar.gz (4.3 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[17 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 2, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/5j/s1w23gm133g69g8vtwz3hg480000gp/T/pip-install-9z_d2lj4/nvidia-cuda-runtime-cu12_d3f62f4e43f245bb815619619ab84145/setup.py\", line 152, in <module>\n",
      "  \u001b[31m   \u001b[0m     raise RuntimeError(open(\"ERROR.txt\", \"r\").read())\n",
      "  \u001b[31m   \u001b[0m RuntimeError:\n",
      "  \u001b[31m   \u001b[0m ###########################################################################################\n",
      "  \u001b[31m   \u001b[0m The package you are trying to install is only a placeholder project on PyPI.org repository.\n",
      "  \u001b[31m   \u001b[0m This package is hosted on NVIDIA Python Package Index.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m This package can be installed as:\n",
      "  \u001b[31m   \u001b[0m ```\n",
      "  \u001b[31m   \u001b[0m $ pip install nvidia-pyindex\n",
      "  \u001b[31m   \u001b[0m $ pip install nvidia-cuda-runtime-cu12\n",
      "  \u001b[31m   \u001b[0m ```\n",
      "  \u001b[31m   \u001b[0m ###########################################################################################\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[?25h\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "\u001b[1;36mhint\u001b[0m: See above for details.\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain-nomic langchain_community tiktoken langchainhub chromadb langchain langgraph tavily-python langchain-text-splitters gpt4all arxiv \"gpt4all[cuda]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# will use llama3 model with Ollama\n",
    "local_llm = 'llama3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"\"\n",
    "os.environ['OPENAI_API_KEY'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Index\n",
    "\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "]\n",
    "\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=250, chunk_overlap=0\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "# Add to vectorDB\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=doc_splits,\n",
    "    collection_name=\"rag-chroma\",\n",
    "    embedding=OpenAIEmbeddings(),\n",
    ")\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"llm agent memory\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Router\n",
    "\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# LLM\n",
    "llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are an expert at routing a\n",
    "    user question to a vectorstore or web search. Use the vectorstore for questions on LLM  agents,\n",
    "    prompt engineering, and adversarial attacks. You do not need to be stringent with the keywords\n",
    "    in the question related to these topics. Otherwise, use web-search. Give a binary choice 'web_search'\n",
    "    or 'vectorstore' based on the question. Return the a JSON with a single key 'datasource' and\n",
    "    no premable or explanation. Question to route: {question} <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"question\"],\n",
    ")\n",
    "\n",
    "question_router = prompt | llm | JsonOutputParser()\n",
    "docs = retriever.get_relevant_documents(question)\n",
    "doc_txt = docs[1].page_content\n",
    "answer = question_router.invoke({\"question\": question})\n",
    "\n",
    "## router!\n",
    "datasource = answer[\"datasource\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grader_prompt():\n",
    "    grader_prompt = PromptTemplate(\n",
    "        template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assessing relevance\n",
    "        of a retrieved document to a user question. If the document contains keywords related to the user question,\n",
    "        grade it as relevant. It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "        Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question. \\n\n",
    "        Provide the binary score as a JSON with a single key 'score' and no premable or explanation.\n",
    "        <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "        Here is the retrieved document: \\n\\n {document} \\n\\n\n",
    "        Here is the user question: {question} \\n <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "        \"\"\",\n",
    "        input_variables=[\"question\", \"document\"],\n",
    "    )\n",
    "\n",
    "    retrieval_grader = grader_prompt | llm | JsonOutputParser()\n",
    "    docs = retriever.invoke(question)\n",
    "    doc_txt = docs[1].page_content\n",
    "    result_grader = retrieval_grader.invoke({\"question\": question, \"document\": doc_txt})\n",
    "    if (result_grader[\"score\"] == \"yes\"):\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "    # 1개 컨텐츠가 아니라 전체 컨텐츠를 보고싶을 때\n",
    "    # result_docs = []\n",
    "    # for doc in docs:\n",
    "    #     result = retrieval_grader.invoke({\"question\": question, \"document\": doc.page_content})\n",
    "    #     if (result['score'] == 'yes'):\n",
    "    #         result_docs.__add__(doc)\n",
    "        \n",
    "    # return result_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(get_grader_prompt())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "def get_gernerated_prompt():\n",
    "    \n",
    "    # Prompt\n",
    "    generated_prompt = PromptTemplate(\n",
    "        template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are an assistant for question-answering tasks.\n",
    "        Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know.\n",
    "        Use three sentences maximum and keep the answer concise <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "        Question: {question}\n",
    "        Context: {context}\n",
    "        Answer: <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "        input_variables=[\"question\", \"document\"],\n",
    "    )\n",
    "\n",
    "    llm = ChatOllama(model=local_llm, temperature=0)\n",
    "\n",
    "    # Chain\n",
    "    rag_chain = generated_prompt | llm | StrOutputParser()\n",
    "\n",
    "    # Run\n",
    "    docs = retriever.invoke(question)\n",
    "    generation = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
    "    return generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know. The provided context does not mention Messi or any information about his current team or playing status. It appears to be a collection of documents discussing large language models, adversarial attacks, and their applications in various fields.\n"
     ]
    }
   ],
   "source": [
    "generation_answer = get_gernerated_prompt()\n",
    "print(generation_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hallucination Grader\n",
    "\n",
    "def get_hallucination_garder(generation):\n",
    "\n",
    "    # LLM\n",
    "    llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n",
    "\n",
    "    # Prompt\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\" <|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assessing whether\n",
    "        an answer is grounded in / supported by a set of facts. Give a binary 'yes' or 'no' score to indicate\n",
    "        whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a\n",
    "        single key 'score' and no preamble or explanation. <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "        Here are the facts:\n",
    "        \\n ------- \\n\n",
    "        {documents}\n",
    "        \\n ------- \\n\n",
    "        Here is the answer: {generation}  <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "        input_variables=[\"generation\", \"documents\"],\n",
    "    )\n",
    "\n",
    "    hallucination_grader = prompt | llm | JsonOutputParser()\n",
    "    return hallucination_grader.invoke({\"documents\": docs, \"generation\": generation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 'no'}\n"
     ]
    }
   ],
   "source": [
    "result = get_hallucination_garder(generation_answer)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Answer Grader\n",
    "\n",
    "def get_answer_grader(generation):\n",
    "    # LLM\n",
    "    llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n",
    "\n",
    "    # Prompt\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assessing whether an\n",
    "        answer is useful to resolve a question. Give a binary score 'yes' or 'no' to indicate whether the answer is\n",
    "        useful to resolve a question. Provide the binary score as a JSON with a single key 'score' and no preamble or explanation.\n",
    "        <|eot_id|><|start_header_id|>user<|end_header_id|> Here is the answer:\n",
    "        \\n ------- \\n\n",
    "        {generation}\n",
    "        \\n ------- \\n\n",
    "        Here is the question: {question} <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "        input_variables=[\"generation\", \"question\"],\n",
    "    )\n",
    "\n",
    "    answer_grader = prompt | llm | JsonOutputParser()\n",
    "    return answer_grader.invoke({\"question\": question, \"generation\": generation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 'yes'}\n"
     ]
    }
   ],
   "source": [
    "answer_garder = get_answer_grader(generation_answer)\n",
    "print(answer_garder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tavily import TavilyClient\n",
    "tavily = TavilyClient(api_key='')\n",
    "\n",
    "def search_query():\n",
    "    \n",
    "    # response = tavily.search(query=question, max_results=3)\n",
    "    # context = [{\"url\": obj[\"url\"], \"content\": obj[\"content\"]} for obj in response['results']]\n",
    "\n",
    "    # You can easily get search result context based on any max tokens straight into your RAG.\n",
    "    # The response is a string of the context within the max_token limit.\n",
    "\n",
    "    # response_context = tavily.get_search_context(query=\"Where does Messi play right now?\", search_depth=\"advanced\", max_tokens=500)\n",
    "\n",
    "    # You can also get a simple answer to a question including relevant sources all with a simple function call:\n",
    "    # You can use it for baseline\n",
    "    return tavily.qna_search(query=question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lionel Messi currently plays for Inter Miami CF in Major League Soccer (MLS) in the United States.\n"
     ]
    }
   ],
   "source": [
    "search_answer = search_query()\n",
    "print(search_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasource:  vectorstore\n",
      "According to the provided context, Component Two of an LLM-powered autonomous agent system is Memory. This component includes Types of Memory and Maximum Inner Product Search (MIPS).\n"
     ]
    }
   ],
   "source": [
    "print(\"datasource: \", datasource)\n",
    "\n",
    "# vectorstore에 있는 내용과 유사하다면 document에서 검색 (related to index)\n",
    "if (datasource == \"vectorstore\") :\n",
    "    # retreve documents with grade\n",
    "    \n",
    "\n",
    "    # relavant condition\n",
    "    is_relevant = get_grader_prompt()\n",
    "    if (is_relevant):\n",
    "        # generate answer\n",
    "        generated_answer = get_gernerated_prompt()\n",
    "\n",
    "        # hallucinations condition\n",
    "        is_hallucination_answer = get_hallucination_garder(generated_answer)\n",
    "        if (is_hallucination_answer[\"score\"] == \"no\"):\n",
    "            #answer question\n",
    "            answer = get_answer_grader(generated_answer)\n",
    "            if (answer[\"score\"] == \"yes\"):\n",
    "                # print\n",
    "                print(generated_answer)\n",
    "\n",
    "            else:\n",
    "                print(search_query())\n",
    "        else:\n",
    "            print(search_query())\n",
    "    else:\n",
    "        print(search_query())\n",
    "else:\n",
    "    print(search_query())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
